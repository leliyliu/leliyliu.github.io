<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="record">
<meta property="og:type" content="website">
<meta property="og:title" content="禾声">
<meta property="og:url" content="http://leliyliu.github.io/index.html">
<meta property="og:site_name" content="禾声">
<meta property="og:description" content="record">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="禾声">
<meta name="twitter:description" content="record">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://leliyliu.github.io/">





  <title>禾声</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">禾声</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">in the arm of the angel, fly away</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2020/03/30/the-one-code-2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/30/the-one-code-2/" itemprop="url">the-one-code-2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-30T15:07:07+08:00">
                2020-03-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="ONE程序代码简析"><a href="#ONE程序代码简析" class="headerlink" title="ONE程序代码简析"></a>ONE程序代码简析</h1><h2 id="INTERFACE"><a href="#INTERFACE" class="headerlink" title="INTERFACE"></a>INTERFACE</h2><p>在INTERFACE 包中一共有五个.java文件，其和core中的NetworkInterface 文件一同构成了整个INTERFACE(ONE 的网络接口部分)</p>
<h3 id="NetworkInterface"><a href="#NetworkInterface" class="headerlink" title="NetworkInterface"></a>NetworkInterface</h3><p>NetwordInterface 是ModuleCommunicationListener 接口的实现。</p>
<h3 id="ConnectivityGrid"><a href="#ConnectivityGrid" class="headerlink" title="ConnectivityGrid"></a>ConnectivityGrid</h3><p>ConnectivityGrid 是继承 ConnectivityOptimizer 的类，ConnnectivityOptimizer是一个抽象类。</p>
<p>不是检测所有的interface ，而只找到那些离得足够近的interface，关于距离足够近，是看两个interface 是否在同一个cell或者相邻的cell中。一个要点是，这个类不支持任何的负坐标。</p>
<p>关于optimizer 的设置，其在default_setting.txt中为：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## Optimization settings -- these affect the speed of the simulation</span><br><span class="line">#<span class="meta"># see World class for details.</span></span><br><span class="line">Optimization.cellSizeMult = <span class="number">5</span></span><br><span class="line">Optimization.randomizeUpdateOrder = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>关于getNeighborCells 方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> GridCell[] getNeighborCells(<span class="keyword">int</span> row, <span class="keyword">int</span> col) &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> GridCell[] &#123;</span><br><span class="line">		cells[row-<span class="number">1</span>][col-<span class="number">1</span>],cells[row-<span class="number">1</span>][col],cells[row-<span class="number">1</span>][col+<span class="number">1</span>],<span class="comment">//1st row</span></span><br><span class="line">		cells[row][col-<span class="number">1</span>],cells[row][col],cells[row][col+<span class="number">1</span>],<span class="comment">//2nd row</span></span><br><span class="line">		cells[row+<span class="number">1</span>][col-<span class="number">1</span>],cells[row+<span class="number">1</span>][col],cells[row+<span class="number">1</span>][col+<span class="number">1</span>]<span class="comment">//3rd row</span></span><br><span class="line">	&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，实际上找的是其自身以及八邻接。</p>
<p>对于cell，这里都是通过坐标来进行索引查找的。</p>
<h3 id="ConnectivityOptimizer"><a href="#ConnectivityOptimizer" class="headerlink" title="ConnectivityOptimizer"></a>ConnectivityOptimizer</h3><p>抽象类，主要包括了如下五个抽象方法，都需要继承的子类进行具体实现：</p>
<ul>
<li><p>addInterface(NetworkInterface ni); </p>
<p>将一个network interface 加入到optimizer当中</p>
</li>
<li><p>addInterfaces(Collection<networkinterface> interfaces)</networkinterface></p>
<p>加入network interface 的 connections</p>
</li>
<li><p>updateLocation(NetworkInterface)</p>
<p>更新network interface 的 地址</p>
</li>
<li><p>Collection<networkinterface> getNearInterfaces(NetworkInterface ni)</networkinterface></p>
<p>找到所有的可能的network interface 使得其能够与当前的Interface进行连接</p>
</li>
<li><p>Collection<networkinterface> getAllInterfaces()</networkinterface></p>
<p>找到所有属于当前connectivityOptimizer的interface</p>
</li>
</ul>
<h3 id="DistanceCapacityInterface"><a href="#DistanceCapacityInterface" class="headerlink" title="DistanceCapacityInterface"></a>DistanceCapacityInterface</h3><p>继承自Network Interface 。 通过interface 之间连接的距离长短来决定links 的能力。基于距离的传输速率在default_setting 中被transmit_speed 设置。</p>
<p>关于interface 直接进行连接</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">(NetworkInterface anotherInterface)</span> </span>&#123;<span class="comment">// 进行连接</span></span><br><span class="line">	<span class="keyword">if</span> (isScanning()</span><br><span class="line">			&amp;&amp; anotherInterface.getHost().isRadioActive()</span><br><span class="line">			&amp;&amp; isWithinRange(anotherInterface)</span><br><span class="line">			&amp;&amp; !isConnected(anotherInterface)</span><br><span class="line">			&amp;&amp; (<span class="keyword">this</span> != anotherInterface)) &#123;</span><br><span class="line"></span><br><span class="line">		Connection con = <span class="keyword">new</span> VBRConnection(<span class="keyword">this</span>.host, <span class="keyword">this</span>,</span><br><span class="line">				anotherInterface.getHost(), anotherInterface);</span><br><span class="line">		connect(con,anotherInterface);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>要进行连接的时候，必须要满足这样一些要求： host 是active 的，并且在范围之内且并没有连接。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getTransmitSpeed</span><span class="params">(NetworkInterface ni)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">double</span> distance;</span><br><span class="line">	<span class="keyword">double</span> fractionIndex;</span><br><span class="line">	<span class="keyword">double</span> decimal;</span><br><span class="line">	<span class="keyword">double</span> speed;</span><br><span class="line">	<span class="keyword">int</span> index;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/* distance to the other interface */</span></span><br><span class="line">	distance = ni.getLocation().distance(<span class="keyword">this</span>.getLocation());</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (distance &gt;= <span class="keyword">this</span>.transmitRange) &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/* interpolate between the two speeds */</span></span><br><span class="line">	fractionIndex = (distance / <span class="keyword">this</span>.transmitRange) *</span><br><span class="line">			(<span class="keyword">this</span>.transmitSpeeds.length - <span class="number">1</span>);</span><br><span class="line">	index = (<span class="keyword">int</span>)(fractionIndex);</span><br><span class="line">	decimal = fractionIndex - index;</span><br><span class="line"></span><br><span class="line">	speed = <span class="keyword">this</span>.transmitSpeeds[index] * (<span class="number">1</span>-decimal) +</span><br><span class="line">			<span class="keyword">this</span>.transmitSpeeds[index + <span class="number">1</span>] * decimal;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> (<span class="keyword">int</span>)speed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据距离来判定当前传输的速率，如果超过范围，则为0，否则会根据当前的distance 和 transmitRange 来进行加权计算。</p>
<h3 id="InterferenceLimitedInterface"><a href="#InterferenceLimitedInterface" class="headerlink" title="InterferenceLimitedInterface"></a>InterferenceLimitedInterface</h3><p>同样继承自Network Interface，是一个简单的Network Interface  提供了一个可变的位速率，基于其它的传输stations（在range范围之内），其与distanceCapacityInterface 的不同在于其基于的是其它station 来决定其bit-rate 的。</p>
<p>其传输速率的计算基于如下方式：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">numberOfTransmissions = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> numberOfActive = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (Connection con : <span class="keyword">this</span>.connections) &#123;</span><br><span class="line">	<span class="keyword">if</span> (con.getMessage() != <span class="keyword">null</span>) &#123;</span><br><span class="line">		numberOfTransmissions++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (((InterferenceLimitedInterface)con.getOtherInterface(<span class="keyword">this</span>)).</span><br><span class="line">			isTransferring() == <span class="keyword">true</span>) &#123;</span><br><span class="line">		numberOfActive++;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> ntrans = numberOfTransmissions;</span><br><span class="line"><span class="keyword">if</span> ( numberOfTransmissions &lt; <span class="number">1</span>) ntrans = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> ( numberOfActive &lt;<span class="number">2</span> ) numberOfActive = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Based on the equation of Gupta and Kumar - and the transmission speed</span></span><br><span class="line"><span class="comment">// is divided equally to all the ongoing transmissions</span></span><br><span class="line">currentTransmitSpeed = (<span class="keyword">int</span>)Math.floor((<span class="keyword">double</span>)transmitSpeed /</span><br><span class="line">		(Math.sqrt((<span class="number">1.0</span>*numberOfActive) *</span><br><span class="line">				Math.log(<span class="number">1.0</span>*numberOfActive))) /</span><br><span class="line">					ntrans );</span><br></pre></td></tr></table></figure>
<p>也就是说，当前的<code>transmitspeed</code>为 : $speed = \frac{speed}{\sqrt{active <em>\log(active)} </em> ntrans}$</p>
<h3 id="SimpleBroadcastInterface"><a href="#SimpleBroadcastInterface" class="headerlink" title="SimpleBroadcastInterface"></a>SimpleBroadcastInterface</h3><p>这是一个很简单的Network Interface ，其提供了一个固定传输速率的服务，即直接简单广播。其余的和前面的interface 实现差不多。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2020/03/24/the-one-code-1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/24/the-one-code-1/" itemprop="url">the-one code 1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-03-24T17:49:10+08:00">
                2020-03-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="ONE程序代码简析"><a href="#ONE程序代码简析" class="headerlink" title="ONE程序代码简析"></a>ONE程序代码简析</h1><hr>
<h2 id="程序执行主流程"><a href="#程序执行主流程" class="headerlink" title="程序执行主流程"></a>程序执行主流程</h2><h3 id="主入口函数"><a href="#主入口函数" class="headerlink" title="主入口函数"></a>主入口函数</h3><p>程序的入口函数在core包中的DTNSim.java文件中，其具体为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">boolean</span> batchMode = <span class="keyword">false</span>;</span><br><span class="line">	<span class="keyword">int</span> nrofRuns[] = &#123;<span class="number">0</span>,<span class="number">1</span>&#125;;</span><br><span class="line">	String confFiles[];</span><br><span class="line">	<span class="keyword">int</span> firstConfIndex = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> guiIndex = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/* set US locale to parse decimals in consistent way */</span></span><br><span class="line">	java.util.Locale.setDefault(java.util.Locale.US);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (args.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (args[<span class="number">0</span>].equals(BATCH_MODE_FLAG)) &#123;</span><br><span class="line">			batchMode = <span class="keyword">true</span>;</span><br><span class="line">               <span class="keyword">if</span> (args.length == <span class="number">1</span>) &#123;</span><br><span class="line">                   firstConfIndex = <span class="number">1</span>;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">else</span> &#123;</span><br><span class="line">                   nrofRuns = parseNrofRuns(args[<span class="number">1</span>]);</span><br><span class="line">                   firstConfIndex = <span class="number">2</span>;</span><br><span class="line">               &#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span> &#123; <span class="comment">/* GUI mode */</span></span><br><span class="line">			<span class="keyword">try</span> &#123; <span class="comment">/* is there a run index for the GUI mode ? */</span></span><br><span class="line">				guiIndex = Integer.parseInt(args[<span class="number">0</span>]);</span><br><span class="line">				firstConfIndex = <span class="number">1</span>;</span><br><span class="line">			&#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</span><br><span class="line">				firstConfIndex = <span class="number">0</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		confFiles = args;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> &#123;</span><br><span class="line">		confFiles = <span class="keyword">new</span> String[] &#123;<span class="keyword">null</span>&#125;;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	initSettings(confFiles, firstConfIndex);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (batchMode) &#123;</span><br><span class="line">		<span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i=nrofRuns[<span class="number">0</span>]; i&lt;nrofRuns[<span class="number">1</span>]; i++) &#123;</span><br><span class="line">			print(<span class="string">"Run "</span> + (i+<span class="number">1</span>) + <span class="string">"/"</span> + nrofRuns[<span class="number">1</span>]);</span><br><span class="line">			Settings.setRunIndex(i);</span><br><span class="line">			resetForNextRun();</span><br><span class="line">			<span class="keyword">new</span> DTNSimTextUI().start();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">double</span> duration = (System.currentTimeMillis() - startTime)/<span class="number">1000.0</span>;</span><br><span class="line">		print(<span class="string">"---\nAll done in "</span> + String.format(<span class="string">"%.2f"</span>, duration) + <span class="string">"s"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> &#123;</span><br><span class="line">		Settings.setRunIndex(guiIndex);</span><br><span class="line">		<span class="keyword">new</span> DTNSimGUI().start();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整体而言，整个main函数做了如下的工作，包括首先初始化某些参数，然后根据传输的参数，选择模式，然后，调用initSetting()函数进行相应的设置，然后则开始run整个函数，下面主要分析一下<code>initSetting</code>函数和<code>Setting</code></p>
<h4 id="initSetting函数"><a href="#initSetting函数" class="headerlink" title="initSetting函数"></a>initSetting函数</h4><p>此函数需要两个参数，<code>confFiles</code>和<code>firstIndex</code>两个参数，其中第一个是string型，第二个为int类型，根据相应的参数，initSetting函数会调用Setting类中的相应方法，Setting.java主要是读取default_setting.txt中的内容的函数，具体进行分析。</p>
<p>在setting 中的init函数中，调用了<code>defProperties.load(new FileInputStream(DEF_SETTINGS_FILE));</code>。这个是用来读取整个txt文档并进行配置的，是整个ONE工程中一个重要的部分。</p>
<h5 id="load-与-load0"><a href="#load-与-load0" class="headerlink" title="load 与 load0"></a>load 与 load0</h5><p>下面具体进行关于load方法的分析，其实质上是调用load0方法（load只是做了一个检测），下面主要分析load0 的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">load0</span><span class="params">(LineReader lr)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    StringBuilder outBuffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    <span class="keyword">int</span> limit;</span><br><span class="line">    <span class="keyword">int</span> keyLen;</span><br><span class="line">    <span class="keyword">int</span> valueStart;</span><br><span class="line">    <span class="keyword">boolean</span> hasSep;</span><br><span class="line">    <span class="keyword">boolean</span> precedingBackslash;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> ((limit = lr.readLine()) &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        keyLen = <span class="number">0</span>;</span><br><span class="line">        valueStart = limit;</span><br><span class="line">        hasSep = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//System.out.println("line=&lt;" + new String(lineBuf, 0, limit) + "&gt;");</span></span><br><span class="line">        precedingBackslash = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (keyLen &lt; limit) &#123;</span><br><span class="line">            <span class="keyword">char</span> c = lr.lineBuf[keyLen];</span><br><span class="line">            <span class="comment">//need check if escaped.</span></span><br><span class="line">            <span class="keyword">if</span> ((c == <span class="string">'='</span> ||  c == <span class="string">':'</span>) &amp;&amp; !precedingBackslash) &#123;</span><br><span class="line">                valueStart = keyLen + <span class="number">1</span>;</span><br><span class="line">                hasSep = <span class="keyword">true</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((c == <span class="string">' '</span> || c == <span class="string">'\t'</span> ||  c == <span class="string">'\f'</span>) &amp;&amp; !precedingBackslash) &#123;</span><br><span class="line">                valueStart = keyLen + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="string">'\\'</span>) &#123;</span><br><span class="line">                precedingBackslash = !precedingBackslash;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                precedingBackslash = <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            keyLen++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (valueStart &lt; limit) &#123;</span><br><span class="line">            <span class="keyword">char</span> c = lr.lineBuf[valueStart];</span><br><span class="line">            <span class="keyword">if</span> (c != <span class="string">' '</span> &amp;&amp; c != <span class="string">'\t'</span> &amp;&amp;  c != <span class="string">'\f'</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (!hasSep &amp;&amp; (c == <span class="string">'='</span> ||  c == <span class="string">':'</span>)) &#123;</span><br><span class="line">                    hasSep = <span class="keyword">true</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            valueStart++;</span><br><span class="line">        &#125;</span><br><span class="line">        String key = loadConvert(lr.lineBuf, <span class="number">0</span>, keyLen, outBuffer);</span><br><span class="line">        String value = loadConvert(lr.lineBuf, valueStart, limit - valueStart, outBuffer);</span><br><span class="line">        put(key, value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整个Properties是从HashTable继承而来的，所以实际上，可以理解为什么ONE 使 用的配置形式是“key = value”。可以看到，判断当前的: 或者=符号来分割key 和 value。每一行进行相应处理，利用loadConvert()方法来进行从参数到具体setting 的转换。</p>
<p>关于内容的保存，其实质上用的是stack（在Setting中声明如下）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Stack&lt;String&gt; oldNamespaces;</span><br><span class="line"><span class="keyword">private</span> Stack&lt;String&gt; secondaryNamespaces;</span><br></pre></td></tr></table></figure>
<p><strong>使用stack保存？？？为什么要用stack，不是很懂</strong></p>
<h5 id="关于Setting中的反射机制"><a href="#关于Setting中的反射机制" class="headerlink" title="关于Setting中的反射机制"></a>关于Setting中的反射机制</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	<span class="function"><span class="keyword">public</span> Object <span class="title">createIntializedObject</span><span class="params">(String className)</span> </span>&#123;</span><br><span class="line">		Class&lt;?&gt;[] argsClass = &#123;Settings.class&#125;;</span><br><span class="line">		Object[] args = &#123;<span class="keyword">this</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> loadObject(className, argsClass, args);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">Class&lt;?&gt; objClass = getClass(className);<span class="comment">//通过getClass来创建相应的类对象，获得相应的反射</span></span><br></pre></td></tr></table></figure>
<p>方法<code>createIntializeObject</code> 或<code>createObject</code> 利用反射机制，将相应的参数传出，利用反射机制动态创建配置文件给出的className对象 </p>
<h3 id="start"><a href="#start" class="headerlink" title="start()"></a>start()</h3><p>在进行完初始化的操作之后(<code>init_setting</code>)，main函数会调用setRunIndex 和 start() 两个函数来进行后续的处理，即开始整个程序的具体运行，有两种不同模式，分别为：DTNSimTextUI 和 DTNSimGUI </p>
<p>整个函数只是调用了两个函数<code>initModel()</code> 和 <code>runSim()</code>，两个函数的作用都还很简单，分别是初始化仿真模型和进行仿真，碧昂且<code>runSim</code>是一个虚函数，是一个abstract，所以实际上需要针对不同的场景做实现。</p>
<h5 id="initModel"><a href="#initModel" class="headerlink" title="initModel"></a>initModel</h5><p>初始化模型这一函数会调用SimScenario.getInstance()这一函数来进行整个模拟场景和节点属性的初始化。</p>
<p>实际上就是new 一个SimScenario 类，具体而言，对于SimScenario类，其构造函数的一些关键部分有：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Settings s = <span class="keyword">new</span> Settings(SCENARIO_NS);<span class="comment">//"Scenario"</span></span><br><span class="line">createHosts();</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.world = <span class="keyword">new</span> World(hosts, worldSizeX, worldSizeY, updateInterval,updateListeners, simulateConnections,</span><br><span class="line">eqHandler.getEventQueues());</span><br></pre></td></tr></table></figure>
<p>可以看到，其实际上调用了createHosts() 和 World() 的构造函数来完成初始化场景和节点的功能，而其中，所有的属性信息都是通过配置文件并利用<code>Setting.java</code>来完成的。</p>
<p><strong>关于DTNHost</strong>：它是整个ONE系统的核心，连接了节点移动模型、通信通道以及路由方式这三大重要模块，并负责这几个模块间的通信任务（利用给出的comBus）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> nextAddress = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> address;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Coord location; 	<span class="comment">// where is the host</span></span><br><span class="line"><span class="keyword">private</span> Coord destination;	<span class="comment">// where is it going</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> MessageRouter router; <span class="comment">//路由</span></span><br><span class="line"><span class="keyword">private</span> MovementModel movement; <span class="comment">// 移动</span></span><br><span class="line"><span class="keyword">private</span> Path path;<span class="comment">//路径</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">double</span> speed;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">double</span> nextTimeToMove;</span><br><span class="line"><span class="keyword">private</span> String name;</span><br><span class="line"><span class="keyword">private</span> List&lt;MessageListener&gt; msgListeners;</span><br><span class="line"><span class="keyword">private</span> List&lt;MovementListener&gt; movListeners;</span><br><span class="line"><span class="keyword">private</span> List&lt;NetworkInterface&gt; net;<span class="comment">//网络接口</span></span><br><span class="line"><span class="keyword">private</span> ModuleCommunicationBus comBus;</span><br></pre></td></tr></table></figure>
<h6 id="createHosts"><a href="#createHosts" class="headerlink" title="createHosts()"></a>createHosts()</h6><p>对于createHosts这一函数的调用，实际上是创建了相应的interface 和 movementModel 以及相应的router，故而实际上其就是一个整体的节点移动模型通信模型和路由方式的创建。</p>
<h6 id="World"><a href="#World" class="headerlink" title="World()"></a>World()</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">World</span><span class="params">(List&lt;DTNHost&gt; hosts, <span class="keyword">int</span> sizeX, <span class="keyword">int</span> sizeY,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">double</span> updateInterval, List&lt;UpdateListener&gt; updateListeners,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">boolean</span> simulateConnections, List&lt;EventQueue&gt; eventQueues)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">this</span>.hosts = hosts;</span><br><span class="line">	<span class="keyword">this</span>.sizeX = sizeX;</span><br><span class="line">	<span class="keyword">this</span>.sizeY = sizeY;</span><br><span class="line">	<span class="keyword">this</span>.updateInterval = updateInterval;</span><br><span class="line">	<span class="keyword">this</span>.updateListeners = updateListeners;</span><br><span class="line">	<span class="keyword">this</span>.simulateConnections = simulateConnections;</span><br><span class="line">	<span class="keyword">this</span>.eventQueues = eventQueues;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">this</span>.simClock = SimClock.getInstance();</span><br><span class="line">	<span class="keyword">this</span>.scheduledUpdates = <span class="keyword">new</span> ScheduledUpdatesQueue();</span><br><span class="line">	<span class="keyword">this</span>.isCancelled = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">	setNextEventQueue();</span><br><span class="line">	initSettings();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整个world 的构造函数，可以看到，实际上是规定了相应的包括相应的事件队列(event queue) 和 监听器等(listener)。</p>
<h5 id="runSim"><a href="#runSim" class="headerlink" title="runSim"></a>runSim</h5><p>runSim 函数实际上是DTNSimGUI中的runSim函数，其调用startGUI() 函数和后续的world.update()函数，实际上，整个world.update()函数是根据时间驱动的，具体为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (simTime &lt; endTime &amp;&amp; !simCancelled)&#123;</span><br><span class="line">	<span class="keyword">if</span> (guiControls.isPaused()) &#123;</span><br><span class="line">		wait(<span class="number">10</span>); <span class="comment">// release CPU resources when paused</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			world.update();</span><br><span class="line">		&#125; <span class="keyword">catch</span> (AssertionError e) &#123;</span><br><span class="line">			<span class="comment">// handles both assertion errors and SimErrors</span></span><br><span class="line">			processAssertionError(e);</span><br><span class="line">		&#125;</span><br><span class="line">		simTime = SimClock.getTime();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">this</span>.update(<span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，当没有设置pause的时候，会根据时间进行world.update()函数的调用，调用之后，更新当前时间simTime。</p>
<p>在整个world.update()的函数中，包括了这样两个主要的函数，即moveHosts() 和updateHosts()。</p>
<p>moveHosts()：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">moveHosts</span><span class="params">(<span class="keyword">double</span> timeIncrement)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>,n = hosts.size(); i&lt;n; i++) &#123;</span><br><span class="line">		DTNHost host = hosts.get(i);</span><br><span class="line">		host.move(timeIncrement);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>也就是每一个时间增量进行一个结点的移动。</p>
<p>updateHosts()函数是一个关键的函数，其调用了<code>hosts.get(i).update(simulateConnections);</code>，其在default_settings.txt中设定为：<code>Scenario.simulateConnections = true</code></p>
<p>整个update 为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(<span class="keyword">boolean</span> simulateConnections)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (!isRadioActive()) &#123;</span><br><span class="line">		<span class="comment">// Make sure inactive nodes don't have connections</span></span><br><span class="line">		tearDownAllConnections();</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (simulateConnections) &#123;</span><br><span class="line">		<span class="keyword">for</span> (NetworkInterface i : net) &#123;</span><br><span class="line">			i.update();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">this</span>.router.update();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也就是会对每一个网络接口和每一个router（路由）进行更新。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过前面的分析，我们大概能知道一个从main()函数方法入口进入并进行整个项目调用的流程，但是我们忽略了很多细节，包括movement ， interface 以及router，还有之间相互交互的信息等。</p>
<p>从整体而言，整个项目的执行过程即为：通过Setting 根据default_setting.txt 文件中读取相应的内容并进行初始化设置，然后会初始化整个模型，包括了模型的节点的移动模型，通信模型，路由方式等。之后开始具体运行，在运行过程中，其主要是通过update进行更新，更新的时间会提前设定。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2020/02/24/pytorch3/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/24/pytorch3/" itemprop="url">pytorch3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-24T11:31:58+08:00">
                2020-02-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pytorch3"><a href="#pytorch3" class="headerlink" title="pytorch3"></a>pytorch3</h1><hr>
<h2 id="批量归一化和残差网络"><a href="#批量归一化和残差网络" class="headerlink" title="批量归一化和残差网络"></a>批量归一化和残差网络</h2><p>利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<h3 id="对全连接层做批量归一化"><a href="#对全连接层做批量归一化" class="headerlink" title="对全连接层做批量归一化"></a>对全连接层做批量归一化</h3><p>位置：全连接层中的仿射变换和激活函数之间。<br><strong>全连接：</strong>  </p>
<script type="math/tex; mode=display">
\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}} \\
 output =\phi(\boldsymbol{x})</script><p><strong>批量归一化：</strong></p>
<script type="math/tex; mode=display">
output=\phi(\text{BN}(\boldsymbol{x}))</script><script type="math/tex; mode=display">
\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})</script><script type="math/tex; mode=display">
\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)},</script><script type="math/tex; mode=display">
\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,</script><script type="math/tex; mode=display">
\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},</script><p>这⾥ϵ &gt; 0是个很小的常数，保证分母大于0</p>
<script type="math/tex; mode=display">
{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot
\hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.</script><p>引入可学习参数：拉伸参数γ和偏移参数β。若$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$，批量归一化无效。</p>
<h3 id="对卷积层做批量归⼀化"><a href="#对卷积层做批量归⼀化" class="headerlink" title="对卷积层做批量归⼀化"></a>对卷积层做批量归⼀化</h3><p>位置：卷积计算之后、应⽤激活函数之前。<br>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。<br>计算：对单通道，batchsize=m,卷积计算输出=pxq<br>对该通道中m×p×q个元素同时做批量归一化,使用相同的均值和方差。</p>
<h3 id="预测时的批量归⼀化"><a href="#预测时的批量归⼀化" class="headerlink" title="预测时的批量归⼀化"></a>预测时的批量归⼀化</h3><p>训练：以batch为单位,对每个batch计算均值和方差。<br>预测：用移动平均估算整个训练数据集的样本均值和方差。</p>
<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。</p>
<h4 id="残差块（Residual-Block）"><a href="#残差块（Residual-Block）" class="headerlink" title="残差块（Residual Block）"></a>残差块（Residual Block）</h4><p>恒等映射：<br>左边：f(x)=x<br>右边：f(x)-x=0 （易于捕捉恒等映射的细微波动）</p>
<p><img src="https://cdn.kesci.com/upload/image/q5l8lhnot4.png?imageView2/0/w/600/h/600" alt="Image Name"></p>
<p>在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。</p>
<h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><h3 id="优化与估计"><a href="#优化与估计" class="headerlink" title="优化与估计"></a>优化与估计</h3><p>尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。</p>
<ul>
<li>优化方法目标：训练集损失函数值</li>
<li>深度学习目标：测试集损失函数值（泛化性）</li>
</ul>
<h3 id="优化在深度学习中的挑战"><a href="#优化在深度学习中的挑战" class="headerlink" title="优化在深度学习中的挑战"></a>优化在深度学习中的挑战</h3><h4 id="局部最小值"><a href="#局部最小值" class="headerlink" title="局部最小值"></a>局部最小值</h4><h4 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h4><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><h3 id="凸性"><a href="#凸性" class="headerlink" title="凸性"></a>凸性</h3><p>在凸性下，优化变得容易被解决。</p>
<h4 id="Jensen-不等式"><a href="#Jensen-不等式" class="headerlink" title="Jensen 不等式"></a>Jensen 不等式</h4><script type="math/tex; mode=display">
\sum_{i} \alpha_{i} f\left(x_{i}\right) \geq f\left(\sum_{i} \alpha_{i} x_{i}\right) \text { and } E_{x}[f(x)] \geq f\left(E_{x}[x]\right)</script><p>即函数值的期望大于期望的函数值</p>
<p>$f^{‘’}(x) \ge 0 \Longleftrightarrow f(x)$ 是凸函数</p>
<p><strong>必要性 ($\Leftarrow$):</strong></p>
<p>对于凸函数：</p>
<script type="math/tex; mode=display">
\frac{1}{2} f(x+\epsilon)+\frac{1}{2} f(x-\epsilon) \geq f\left(\frac{x+\epsilon}{2}+\frac{x-\epsilon}{2}\right)=f(x)</script><p>故:</p>
<script type="math/tex; mode=display">
f^{\prime \prime}(x)=\lim _{\varepsilon \rightarrow 0} \frac{\frac{f(x+\epsilon) - f(x)}{\epsilon}-\frac{f(x) - f(x-\epsilon)}{\epsilon}}{\epsilon}</script><script type="math/tex; mode=display">
f^{\prime \prime}(x)=\lim _{\varepsilon \rightarrow 0} \frac{f(x+\epsilon)+f(x-\epsilon)-2 f(x)}{\epsilon^{2}} \geq 0</script><p><strong>充分性 ($\Rightarrow$):</strong></p>
<p>令 $a &lt; x &lt; b$ 为 $f(x)$ 上的三个点，由拉格朗日中值定理:</p>
<script type="math/tex; mode=display">
\begin{array}{l}{f(x)-f(a)=(x-a) f^{\prime}(\alpha) \text { for some } \alpha \in[a, x] \text { and }} \\ {f(b)-f(x)=(b-x) f^{\prime}(\beta) \text { for some } \beta \in[x, b]}\end{array}</script><p>根据单调性，有 $f^{\prime}(\beta) \geq f^{\prime}(\alpha)$, 故:</p>
<script type="math/tex; mode=display">
\begin{aligned} f(b)-f(a) &=f(b)-f(x)+f(x)-f(a) \\ &=(b-x) f^{\prime}(\beta)+(x-a) f^{\prime}(\alpha) \\ & \geq(b-a) f^{\prime}(\alpha) \end{aligned}</script><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>在 $x + \epsilon$ 处泰勒展开：</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\epsilon)=f(\mathbf{x})+\epsilon^{\top} \nabla f(\mathbf{x})+\frac{1}{2} \epsilon^{\top} \nabla \nabla^{\top} f(\mathbf{x}) \epsilon+\mathcal{O}\left(\|\epsilon\|^{3}\right)</script><p>最小值点处满足: $\nabla f(\mathbf{x})=0$, 即我们希望 $\nabla f(\mathbf{x} + \epsilon)=0$, 对上式关于 $\epsilon$ 求导，忽略高阶无穷小，有：</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x})+\boldsymbol{H}_{f} \boldsymbol{\epsilon}=0 \text { and hence } \epsilon=-\boldsymbol{H}_{f}^{-1} \nabla f(\mathbf{x})</script><h3 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h3><script type="math/tex; mode=display">
\begin{array}{ll}{\eta(t)=\eta_{i} \text { if } t_{i} \leq t \leq t_{i+1}} & {\text { piecewise constant }} \\ {\eta(t)=\eta_{0} \cdot e^{-\lambda t}} & {\text { exponential }} \\ {\eta(t)=\eta_{0} \cdot(\beta t+1)^{-\alpha}} & {\text { polynomial }}\end{array}</script><h2 id="补充：矩阵微积分"><a href="#补充：矩阵微积分" class="headerlink" title="补充：矩阵微积分"></a>补充：矩阵微积分</h2><p>在数学中，矩阵微积分是进行多变量微积分的一种特殊符号，特别是在矩阵的空间上。  它将关于许多变量的单个函数的各种偏导数和/或关于单个变量的多变量函数的偏导数收集到可以被视为单个实体的向量和矩阵中。  这大大简化例如找到多元函数的最大值或最小值，以及求解微分方程组的操作。  这里使用的符号通常用于统计和工程中，而张量指数符号在物理学中是比较常用的。 </p>
<p>矩阵微积分是指一组不同的符号，这组符号使用矩阵和向量来收集因变量的每个分量相对于自变量的每个分量的导数。总的来说，自变量可以是标量，向量，或者是一个矩阵，因变量也可以是上述的三者之一。每一种不同的自变量和因变量的组合都有不同的一套运算规则。</p>
<p>前面两种情况中的每一种都可以被认为是使用适当大小的向量对向量求导数的应用。类似地，我们将发现涉及矩阵的导数将以相应的方式减少到涉及向量的导数。<br> 对函数向量(一个向量其组成是函数)$\mathbf{y}=\left[\begin{aligned}\begin{matrix}y_1\\y_2\\\vdots\\y_m\end{matrix}\end{aligned}\right]$，对一个输入向量$\mathbf{x}=\left[\begin{aligned}\begin{matrix}x_1\\x_2\\\vdots\\x_m\end{matrix}\end{aligned}\right]$求导（使用分子布局），可以写成： </p>
<script type="math/tex; mode=display">
\dfrac{\partial\mathbf{y}}{\partial\mathbf{x}}=
\left[
\begin{aligned}
\begin{matrix}
\dfrac{\partial{y_1}}{\partial{x_1}}&\dfrac{\partial{y_1}}{\partial{x_2}}&\cdots&\dfrac{\partial{y_1}}{\partial{x_n}}\\
\dfrac{\partial{y_2}}{\partial{x_1}}&\dfrac{\partial{y_2}}{\partial{x_2}}&\cdots&\dfrac{\partial{y_2}}{\partial{x_n}}\\
\vdots&\vdots&\ddots&\vdots\\
\dfrac{\partial{y_m}}{\partial{x_1}}&\dfrac{\partial{y_m}}{\partial{x_2}}&\cdots&\dfrac{\partial{y_m}}{\partial{x_n}}\\
\end{matrix}
\end{aligned}
\right]</script><p>这里展示的是基础，关于更多内容，可以参考这篇博客<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a></p>
<h2 id="优化算法进阶"><a href="#优化算法进阶" class="headerlink" title="优化算法进阶"></a>优化算法进阶</h2><p>在 <a href="https://d2l.ai/chapter_optimization/sgd.html#sec-sgd" target="_blank" rel="noopener">Section 11.4</a> 中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。</p>
<script type="math/tex; mode=display">
\mathbf{g}_t = \partial_{\mathbf{w}} \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} f(\mathbf{x}_{i}, \mathbf{w}_{t-1}) = \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} \mathbf{g}_{i, t-1}.</script><h3 id="ill-condition"><a href="#ill-condition" class="headerlink" title="ill-condition"></a>ill-condition</h3><p>Condition Number of Hessian Matrix:</p>
<script type="math/tex; mode=display">
 cond_{H} = \frac{\lambda_{max}}{\lambda_{min}}</script><p>where $\lambda_{max}, \lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.</p>
<p>让我们考虑一个输入和输出分别为二维向量$\boldsymbol{x} = [x_1, x_2]^\top$和标量的目标函数:</p>
<script type="math/tex; mode=display">
 f(\boldsymbol{x})=0.1x_1^2+2x_2^2</script><script type="math/tex; mode=display">
cond_{H} = \frac{4}{0.2} = 20 \quad \rightarrow \quad \text{ill-conditioned}</script><p>对于ill-condition ， 其主要问题在于在不同的维度上，所期望得到的合适的lr是不同的，因此，考虑将其放缩到相近的大小上：</p>
<p>在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \Delta_{x} = H^{-1}\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。</p>
<ul>
<li><strong>Preconditioning gradient vector</strong>: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.</li>
<li><strong>Averaging history gradient</strong>: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.  </li>
</ul>
<h3 id="Momentum-Algorithm"><a href="#Momentum-Algorithm" class="headerlink" title="Momentum Algorithm"></a>Momentum Algorithm</h3><p>动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\boldsymbol{x}_t$，学习率为 $\eta_t$。<br>在时间步 $t=0$，动量法创建速度变量 $\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t&gt;0$，动量法对每次迭代的步骤做如下修改：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{m}_t &\leftarrow \beta \boldsymbol{m}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{m}_t,
\end{aligned}</script><p>Another version:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{m}_t &\leftarrow \beta \boldsymbol{m}_{t-1} + (1-\beta) \boldsymbol{g}_t, \\
\boldsymbol{x}_t &\leftarrow \boldsymbol{x}_{t-1} - \alpha_t \boldsymbol{m}_t,
\end{aligned}</script><script type="math/tex; mode=display">
\alpha_t = \frac{\eta_t}{1-\beta}</script><p>其中，动量超参数 $\beta$满足 $0 \leq \beta &lt; 1$。当 $\beta=0$ 时，动量法等价于小批量随机梯度下降。</p>
<p>在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。</p>
<p>对于动量算法，其主要考虑了当前梯度下降方向前的历史记录对于其的影响，使得其能够在某些情况下避免到ill-condition问题。</p>
<p>可以通过移动加权平均的方式来考察理解动量法。</p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>其实质上是利用考虑Hessian matrix的方式来使得之前的问题得到处理</p>
<h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>AdaGrad算法会使用一个小批量随机梯度$\boldsymbol{g}_t$按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，AdaGrad将$\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</p>
<script type="math/tex; mode=display">
\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,</script><p>其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,</script><p>其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p>
<p>其问题在于下降的后期可能会导致梯度消失</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：</p>
<ol>
<li><a href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html#%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">Skip-Gram 跳字模型</a>：假设背景词由中心词生成，即建模 $P(w_o\mid w_c)$，其中 $w_c$ 为中心词，$w_o$ 为任一背景词；</li>
</ol>
<p><img src="https://cdn.kesci.com/upload/image/q5mjsq84o9.png?imageView2/0/w/960/h/960" alt="Image Name"></p>
<ol>
<li><a href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">CBOW (continuous bag-of-words) 连续词袋模型</a>：假设中心词由背景词生成，即建模 $P(w_c\mid \mathcal{W}_o)$，其中 $\mathcal{W}_o$ 为背景词的集合。</li>
</ol>
<p><img src="https://cdn.kesci.com/upload/image/q5mjt4r02n.png?imageView2/0/w/960/h/960" alt="Image Name"></p>
<h4 id="二次采样"><a href="#二次采样" class="headerlink" title="二次采样"></a>二次采样</h4><p>对于某些高频词，其实际上的意义是不大的，因此需要考虑丢弃这些高频词。采用如下的方式进行二次采样：</p>
<script type="math/tex; mode=display">
P(w_i)=\max(1-\sqrt{\frac{t}{f(w_i)}},0)</script><p>其中  $f(w_i)$  是数据集中词 $w_i$ 的个数与总词数之比，常数 $t$ 是一个超参数（实验中设为 $10^{−4}$）。可见，只有当 $f(w_i)&gt;t$ 时，我们才有可能在二次采样中丢弃词 $w_i$，并且越高频的词被丢弃的概率越大。</p>
<h3 id="Skip-Gram-跳字模型"><a href="#Skip-Gram-跳字模型" class="headerlink" title="Skip-Gram 跳字模型"></a>Skip-Gram 跳字模型</h3><p>在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\boldsymbol{v}_i\in\mathbb{R}^d$，而为背景词时向量表示为 $\boldsymbol{u}_i\in\mathbb{R}^d$ 。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，我们假设给定中心词生成背景词的条件概率满足下式：</p>
<script type="math/tex; mode=display">
P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}</script><p>由于词量较大，因此需要考虑采用负采样的方式来解决此问题：</p>
<p>负采样方法用以下公式来近似条件概率 $P(w_o\mid w_c)=\frac{\exp(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{\sum_{i\in\mathcal{V}}\exp(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}$：</p>
<script type="math/tex; mode=display">
P(w_o\mid w_c)=P(D=1\mid w_c,w_o)\prod_{k=1,w_k\sim P(w)}^K P(D=0\mid w_c,w_k)</script><p>其中 $P(D=1\mid w_c,w_o)=\sigma(\boldsymbol{u}_o^\top\boldsymbol{v}_c)$，$\sigma(\cdot)$ 为 sigmoid 函数。对于一对中心词和背景词，我们从词典中随机采样 $K$ 个噪声词（实验中设 $K=5$）。根据 Word2Vec 论文的建议，噪声词采样概率 $P(w)$ 设为 $w$ 词频与总词频之比的 $0.75$ 次方。</p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。我们将在后面基于锚框实践目标检测。</p>
<p>假设输入图像高为 $h$，宽为$w$。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为$s\in (0,1]$且宽高比为$r &gt; 0$，那么锚框的宽和高将分别为$ws\sqrt{r}$和$hs/\sqrt{r}$。当中心位置给定时，已知宽和高的锚框是确定的。</p>
<p>下面我们分别设定好一组大小$s_1,\ldots,s_n$和一组宽高比$r_1,\ldots,r_m$。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到$whnm$个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含$s_1$或$r_1$的大小与宽高比的组合感兴趣，即</p>
<script type="math/tex; mode=display">
(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).</script><p>也就是说，以相同像素为中心的锚框的数量为$n+m-1$。对于整个输入图像，我们将一共生成$wh(n+m-1)$个锚框。</p>
<h3 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h3><p>我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合$\mathcal{A}$和$\mathcal{B}$，它们的Jaccard系数即二者交集大小除以二者并集大小：</p>
<script type="math/tex; mode=display">
J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.</script><p>实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5vs9jkw9f.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<p>在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，我们根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的预测边界框。常用的方法叫作非极大值抑制（non-maximum suppression，NMS）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2020/02/16/pytorch2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/16/pytorch2/" itemprop="url">pytorch2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-16T19:59:51+08:00">
                2020-02-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="动手学深度学习-task2"><a href="#动手学深度学习-task2" class="headerlink" title="动手学深度学习 task2"></a>动手学深度学习 task2</h1><hr>
<h2 id="梯度消失、梯度爆炸"><a href="#梯度消失、梯度爆炸" class="headerlink" title="梯度消失、梯度爆炸"></a>梯度消失、梯度爆炸</h2><h2 id="机器翻译及相关技术"><a href="#机器翻译及相关技术" class="headerlink" title="机器翻译及相关技术"></a>机器翻译及相关技术</h2><p>机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。</p>
<h3 id="数据预处理过程"><a href="#数据预处理过程" class="headerlink" title="数据预处理过程"></a>数据预处理过程</h3><p>对于一般的数据预处理过程，首先要保证编码方式的正确。然后需要转换大小写，这里没有许多多余的处理操作。</p>
<h4 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br></pre></td></tr></table></figure>
<p>这个pad 函数的作用在于保持每个句子的长度是一样的，如果是大于的话，那么就进行阶段，否则进行相应的补足。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure>
<p>注意这里关于有效长度的计算，也就是计算非补足的长度。</p>
<h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><p>其结构是如此表示的，即所有的可能是以这样一种方式显现出来</p>
<p><img src="https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<p>因为本身的RNN的网络结构不能保持具体的翻译过程中长度的变化，那么要达到这样的目标，使用了这样一种方式，即首先对输入进行了编码，然后进行解码后输出。</p>
<h4 id="Sequence-to-Sequence-模型"><a href="#Sequence-to-Sequence-模型" class="headerlink" title="Sequence to Sequence 模型"></a>Sequence to Sequence 模型</h4><p><img src="https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<p><img src="https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500" alt="Image Name"></p>
<p>这里进行了相应的embedding，是将相应的输入转换为词向量来作为相应的输入。词向量的长度是相同的维度。</p>
<h4 id="训练模型和测试模型"><a href="#训练模型和测试模型" class="headerlink" title="训练模型和测试模型"></a>训练模型和测试模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch7</span><span class="params">(model, data_iter, lr, num_epochs, device)</span>:</span>  <span class="comment"># Saved in d2l</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs+<span class="number">1</span>):</span><br><span class="line">        l_sum, num_tokens_sum = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_vlen, Y, Y_vlen = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            Y_input, Y_label, Y_vlen = Y[:,:<span class="number">-1</span>], Y[:,<span class="number">1</span>:], Y_vlen<span class="number">-1</span></span><br><span class="line">            </span><br><span class="line">            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)</span><br><span class="line">            l = loss(Y_hat, Y_label, Y_vlen).sum()</span><br><span class="line">            l.backward()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                d2l.grad_clipping_nn(model, <span class="number">5</span>, device)</span><br><span class="line">            num_tokens = Y_vlen.sum().item()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.sum().item()</span><br><span class="line">            num_tokens_sum += num_tokens</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch &#123;0:4d&#125;,loss &#123;1:.3f&#125;, time &#123;2:.1f&#125; sec"</span>.format( </span><br><span class="line">                  epoch, (l_sum/num_tokens_sum), time.time()-tic))</span><br><span class="line">            tic = time.time()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_ch7</span><span class="params">(model, src_sentence, src_vocab, tgt_vocab, max_len, device)</span>:</span></span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">' '</span>)]</span><br><span class="line">    src_len = len(src_tokens)</span><br><span class="line">    <span class="keyword">if</span> src_len &lt; max_len:</span><br><span class="line">        src_tokens += [src_vocab.pad] * (max_len - src_len)</span><br><span class="line">    enc_X = torch.tensor(src_tokens, device=device)</span><br><span class="line">    enc_valid_length = torch.tensor([src_len], device=device)</span><br><span class="line">    <span class="comment"># use expand_dim to add the batch_size dimension.</span></span><br><span class="line">    enc_outputs = model.encoder(enc_X.unsqueeze(dim=<span class="number">0</span>), enc_valid_length)</span><br><span class="line">    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)</span><br><span class="line">    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">    predict_tokens = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(max_len):</span><br><span class="line">        Y, dec_state = model.decoder(dec_X, dec_state)</span><br><span class="line">        <span class="comment"># The token with highest score is used as the next time step input.</span></span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        py = dec_X.squeeze(dim=<span class="number">0</span>).int().item()</span><br><span class="line">        <span class="keyword">if</span> py == tgt_vocab.eos:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        predict_tokens.append(py)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">' '</span>.join(tgt_vocab.to_tokens(predict_tokens))</span><br></pre></td></tr></table></figure>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p><img src="https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960" alt="Image Name"></p>
<p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。</p>
<p>为了计算输出，我们首先假设有一个函数$\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \ldots, a_n$ by</p>
<script type="math/tex; mode=display">
a_i = \alpha(\mathbf q, \mathbf k_i).</script><p>我们使用<code>softmax</code>函数 获得注意力权重：</p>
<script type="math/tex; mode=display">
b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).</script><p>最终的输出就是value的加权求和：</p>
<script type="math/tex; mode=display">
\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.</script><h3 id="softmax-屏蔽"><a href="#softmax-屏蔽" class="headerlink" title="softmax 屏蔽"></a>softmax 屏蔽</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>: </span><br><span class="line"><span class="comment">#如果是一维的话，表示没有考虑到有多个batch_size，那么需要进行考虑</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3] 进行repeat 是指需要repeat了相应的步长维度给每个batch</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3] 这里考虑了是在具体哪个device 上进行训练</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br></pre></td></tr></table></figure>
<p>这里的mask 的作用和上一节中实际上是一样的，由于每一句的长度是不同的，但是在进行训练的时候，我们需要保持每一句所展现出来的维度是相同的，故而对于缺少的，会进行相应的padding ,但是补足的部分不应该纳入<code>softmax</code>的计算当中。</p>
<h4 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h4><p>深度学习在自然语言处理方面进行应用的时候，维度是比较复杂的一个点，这里主要进行一下说明。</p>
<p>一般来说，输入的维度一般都包括了这样几个方面，batch_size ，步长，输入维度。对于语言而言，其本身的网络，不仅包括了原来的小批量训练时候的batch_size 和 本来一个语句的维度，同时还有一个时间维度。</p>
<p>例如在进行相应的mask的时候，可以参见上面的代码中所给出的注释的含义。</p>
<h4 id="高维矩阵相乘"><a href="#高维矩阵相乘" class="headerlink" title="高维矩阵相乘"></a>高维矩阵相乘</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bmm(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>), dtype = torch.float), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>), dtype = torch.float)) <span class="comment">#两个矩阵乘法</span></span><br></pre></td></tr></table></figure>
<p><code>torch.bmm</code>是矩阵的乘法，对于高维矩阵，比如上面的代码所展示的那样，得到的shape = (2,1,2)</p>
<p>两个数组尺寸要求：</p>
<ul>
<li>“<strong>2维以上</strong>“的尺寸必须完全对应相等；</li>
<li>“<strong>2维</strong>“具有实际意义的单位，只要满足矩阵相乘的尺寸规律即可。</li>
</ul>
<h3 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h3><p>The dot product 假设query和keys有相同的维度, 即 $\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下</p>
<script type="math/tex; mode=display">
𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \sqrt{d}</script><p>假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：</p>
<script type="math/tex; mode=display">
𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\sqrt{d}</script><p>现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p>
<h3 id="多层感知机注意力"><a href="#多层感知机注意力" class="headerlink" title="多层感知机注意力"></a>多层感知机注意力</h3><p>在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射<br>$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义</p>
<script type="math/tex; mode=display">
𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)</script><p>.<br>然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p>
<h3 id="引入注意力机制的Seq2seq模型"><a href="#引入注意力机制的Seq2seq模型" class="headerlink" title="引入注意力机制的Seq2seq模型"></a>引入注意力机制的Seq2seq模型</h3><p>将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：</p>
<p><img src="https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig1具有注意机制的seq-to-seq模型解码的第二步</script><p>下图展示了seq2seq机制的所有层的关系，下面展示了encoder和decoder的layer结构</p>
<p><img src="https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig2具有注意机制的seq-to-seq模型中层结构</script><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：</p>
<ul>
<li>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</li>
<li>RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。</li>
</ul>
<p>为了整合CNN和RNN的优势，<a href="https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017" target="_blank" rel="noopener">[Vaswani et al., 2017]</a> 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。</p>
<p>图10.3.1展示了Transformer模型的架构，与seq2seq模型相似，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p>
<ol>
<li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li>
<li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li>
<li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li>
</ol>
<p><img src="https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960" alt="Fig. 10.3.1 The Transformer architecture."></p>
<script type="math/tex; mode=display">
Fig.10.3.1\ Transformer 架构.</script><h3 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h3><p>在我们讨论多头注意力层之前，先来迅速理解以下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图10.3.2 自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320" alt="Fig. 10.3.2 自注意力结构"></p>
<script type="math/tex; mode=display">
Fig.10.3.2\ 自注意力结构</script><p>多头注意力层包含$h$个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig.10.3.3\ 多头注意力</script><p>假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \in \mathbb{R}^{p_q\times d_q}$、$W_k^{(i)} \in \mathbb{R}^{p_k\times d_k}$和$W_v^{(i)} \in \mathbb{R}^{p_v\times d_v}$，以得到每个头的输出：</p>
<script type="math/tex; mode=display">
o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)</script><p>这里的attention可以是任意的attention function，比如前一节介绍的dot-product attention以及MLP attention。之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\in \mathbb{R}^{d_0 \times hp_v}$</p>
<script type="math/tex; mode=display">
o = W_o[o^{(1)}, \ldots, o^{(h)}]</script><p>接下来我们就可以来实现多头注意力了，假设我们有h个头，隐藏层权重 $hidden_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature的维度也设置为 $d_0 = hidden_size$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>
<p>关于其中实现的代码，首先要注意其维度上的变化，对于query , key 和 value ，其本身在维度上都是(batch_size , seq_len , dim )，这和之前的是相同的，然后利用transpose 会将最后一维变成两维，倒数第二维是head_nums，然后再进行变换，具体可参见如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>关于<code>np.tile</code>函数：官方文档为</p>
<p><code>Construct an array by repeating A the number of times given by reps.</code></p>
<p>If <em>reps</em> has length <code>d</code>, the result will have dimension of <code>max(d, A.ndim)</code>.</p>
<p>If <code>A.ndim &lt; d</code>, <em>A</em> is promoted to be d-dimensional by prepending new axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication, or shape (1, 1, 3) for 3-D replication. If this is not the desired behavior, promote <em>A</em> to d-dimensions manually before calling this function.</p>
<p>If <code>A.ndim &gt; d</code>, <em>reps</em> is promoted to <em>A</em>.ndim by pre-pending 1’s to it. Thus for an <em>A</em> of shape (2, 3, 4, 5), a <em>reps</em> of (2, 2) is treated as (1, 1, 2, 2).</p>
<p>注意维度的调换</p>
<p>最后的output 的 shape  : (batch_size,seq_len,hide_size * head_nums)</p>
<h3 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h3><p>其效果主要用于变换维度，实际上就等同于一个$1 \times 1$的卷积层</p>
<h3 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h3><p>除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。 <a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">(ref)</a> </p>
<p>与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。</p>
<p>假设输入序列的嵌入表示 $X\in \mathbb{R}^{l\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \in \mathbb{R}^{l\times d}$ ，输出的向量就是二者相加 $X + P$。</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：</p>
<script type="math/tex; mode=display">
P_{i,2j} = sin(i/10000^{2j/d})</script><script type="math/tex; mode=display">
P_{i,2j+1} = cos(i/10000^{2j/d})</script><script type="math/tex; mode=display">
for\ i=0,\ldots, l-1\ and\ j=0,\ldots,\lfloor (d-1)/2 \rfloor</script><p><img src="https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<script type="math/tex; mode=display">
Fig. 10.3.4\ 位置编码</script><p>对于transformer而言，本身没有包含位置的性质。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>在这里，要理解一下，对于训练和预测，其输入是不同的，在训练过程中，会直接输入整个targets的所有tokens，那么这个时候为了保证其不看到后面的结果来进行loss的训练，需要设定当前的<code>valid_length</code>，相当于只看到了seq_len+1 这一个tokens为止。</p>
<h2 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h2><p>输入维度：</p>
<p>对于卷积神经网络来说，一般的输入X和中间的隐藏层，都具有4个维度，其分别是：</p>
<p>(batch_size, channels,length,width)</p>
<p>对于其他部分，应该就比较好理解。</p>
<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><p>LeNet是一个最经典的卷积神经网络。</p>
<p>使用全连接层的局限性：</p>
<ul>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易导致模型过大。</li>
</ul>
<p>使用卷积层的优势：</p>
<ul>
<li>卷积层保留输入形状。</li>
<li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</li>
</ul>
<h2 id="卷积神经网络进阶"><a href="#卷积神经网络进阶" class="headerlink" title="卷积神经网络进阶"></a>卷积神经网络进阶</h2><p>主要介绍了几个经典的卷积神经网络</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。<br> <strong>特征：</strong></p>
<ol>
<li>8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</li>
<li>将sigmoid激活函数改成了更加简单的ReLU激活函数。</li>
<li>用Dropout来控制全连接层的模型复杂度。</li>
<li>引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</li>
</ol>
<h3 id="使用重复元素的网络（VGG）"><a href="#使用重复元素的网络（VGG）" class="headerlink" title="使用重复元素的网络（VGG）"></a>使用重复元素的网络（VGG）</h3><p>VGG：通过重复使⽤简单的基础块来构建深度模型。<br> Block:数个相同的填充为1、窗口形状为</p>
<p>的卷积层,接上一个步幅为2、窗口形状为的最大池化层。<br> 卷积层保持输入的高和宽不变，而池化层则对其减半。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640" alt="Image Name"></p>
<h3 id="网络中的网络-NiN"><a href="#网络中的网络-NiN" class="headerlink" title="网络中的网络(NiN)"></a>网络中的网络(NiN)</h3><p>LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。<br> NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。<br> ⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。</p>
<p><img src="https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960" alt="Image Name"></p>
<h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><ol>
<li>由Inception基础块组成。  </li>
<li>Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   </li>
<li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 </li>
</ol>
<p><img src="https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640" alt="Image Name"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2020/02/14/pytorch1/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/14/pytorch1/" itemprop="url">pytorch1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-14T11:51:18+08:00">
                2020-02-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TOC]</p>
<h1 id="动手学深度学习-task1"><a href="#动手学深度学习-task1" class="headerlink" title="动手学深度学习 task1"></a>动手学深度学习 task1</h1><hr>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>对于简单的线性回归而言，实际上就非常简单，只需要一个线性层即可。对于python而言，一般来说，使用科学计算提供的矢量运算要比利用循环实现的效率高很多。这里实际上跟CPU本身的指令集扩展有关，包括AVX和MME的矢量运算指令。</p>
<h3 id="yield-使用"><a href="#yield-使用" class="headerlink" title="yield 使用"></a>yield 使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># random read 10 samples</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class="comment"># the last time may be not enough for a whole batch</span></span><br><span class="line">        <span class="keyword">yield</span>  features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure>
<p>在这里，使用了yield，这里的yield 有两个作用，包括return 和一个迭代器的作用。在python 中，这样的例子还包括之前有的xrange，即现在的range，如果去打印type(range(10))，可以看到，当前的range返回的也是这样一个迭代器。</p>
<h3 id="小批量计算"><a href="#小批量计算" class="headerlink" title="小批量计算"></a>小批量计算</h3><p>batch normalization 是两种不同方式的中和，既不需要计算整个批量的所有数据来进行梯度下降，也不由于某些极少数的点来导致下降的方向不对。一般而言，小批量的计算是用于对于大量数据的梯度下降， 而对于少量的数据，直接使用批量梯度下降即可。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>一般而言，神经网络都是BP神经网络，也就是反向传播是很重要的一个过程，故而在训练的过程中，需要保持参数的反向传播（能计算其导数），在pytorch中，给出了相应的方法来进行反向传播。</p>
<h2 id="softmax-与分类模型"><a href="#softmax-与分类模型" class="headerlink" title="softmax 与分类模型"></a>softmax 与分类模型</h2><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>softmax的提出，很大程度上是为了数值计算的方便。对于softmax的理解，主要要通过信息论和数值计算这两方面来进行，softmax函数本身所具有的特性，让他能够很好地运用在分类网络中。</p>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>对于交叉熵，这里实际上是信息论的一个重要概念，其本身用在分类模型中，表示我们只关心分类出来的那个结果的概率比的大小，而不在意其它的结果。</p>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p>多层感知机其原理就是通过在一层和一层之间加上激活函数，使得本来很简单的过程可以映射到更复杂的网络结构和高维的数据分布结构当中。</p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="两种不同采样方式下的处理"><a href="#两种不同采样方式下的处理" class="headerlink" title="两种不同采样方式下的处理"></a>两种不同采样方式下的处理</h3><p>在实现RNN 的训练过程中，有这样一段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:  <span class="comment"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class="line">    state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="keyword">if</span> is_random_iter:  <span class="comment"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class="line">        state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">            s.detach_()</span><br></pre></td></tr></table></figure>
<p>对于两种不同的采样方式，采用了不同的对于隐藏状态H 的处理方式，对此进行理解</p>
<h4 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h4><p>对于随机采样，由于采样结果的不连续，那么之前所具有的状态H对当前是没有意义的，故需要重新初始化状态<code>`init_rnn_state</code>。</p>
<h4 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h4><p>在相邻采样开始前，首先进行了<code>init_rnn_state</code> 函数调用，即进行了相应的初始化。由于使用的是相邻采样，那么正如例子所展示的那样，得到的数据应该是相邻连续的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X:  tensor([[ 0,  1,  2,  3,  4,  5],</span><br><span class="line">        [15, 16, 17, 18, 19, 20]]) </span><br><span class="line">Y: tensor([[ 1,  2,  3,  4,  5,  6],</span><br><span class="line">        [16, 17, 18, 19, 20, 21]]) </span><br><span class="line"></span><br><span class="line">X:  tensor([[ 6,  7,  8,  9, 10, 11],</span><br><span class="line">        [21, 22, 23, 24, 25, 26]]) </span><br><span class="line">Y: tensor([[ 7,  8,  9, 10, 11, 12],</span><br><span class="line">        [22, 23, 24, 25, 26, 27]])</span><br></pre></td></tr></table></figure>
<p>所以实际上，进行<code>data_iter_fn</code>迭代的时候，所得到的数据，即是连续的，那么其本身的状态是可以保持的，正如用[0,1,2,3,4,5]的状态推出到[1,2,3,4,5,6]的状态一样，这样的过程是连续进行的，所以状态也应该是连续的。所以这个时候，应该保持状态H不变，而不需要像随机采样那样重新<code>init_rnn_state</code>，所以我们需要使用detach_来保留当前H的状态，来进行后续的推导，那么为什么要使用<code>detach_</code>而不直接保留呢？</p>
<p>可以参见detach_的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># detach_ 的源码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detach_</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Detaches the Variable from the graph that created it, making it a</span></span><br><span class="line"><span class="string">    leaf.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self._grad_fn = <span class="keyword">None</span></span><br><span class="line">    self.requires_grad = <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p>可以看到，这里将其将 Variable 的grad_fn 设置为 None，这样，BP 的时候，到这个 Variable 就找不到 它的 grad_fn，所以就不会再往后BP了。</p>
<p>因为采样的时候，已经预设了相应的前提，即确定的时间步长，不应该再向前BP（反向传播），故而需要将其detach掉。</p>
<p>对于detach_函数的理解，可以参考这篇博客<a href="https://www.cnblogs.com/jiangkejie/p/9981707.html" target="_blank" rel="noopener">pytorch中的detach_和detach</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2019/11/01/hello-world/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/01/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-01T19:51:23+08:00">
                2019-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2019/09/25/github&git/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/25/github&git/" itemprop="url">github&git</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-25T19:15:03+08:00">
                2019-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Git-amp-github"><a href="#Git-amp-github" class="headerlink" title="Git &amp; github"></a>Git &amp; github</h1><hr>
<h2 id="github-账号申请"><a href="#github-账号申请" class="headerlink" title="github 账号申请"></a>github 账号申请</h2><h3 id="personal-or-workgroup"><a href="#personal-or-workgroup" class="headerlink" title="personal or workgroup"></a>personal or workgroup</h3><ul>
<li>个人账号注册-&gt; 登录<a href="https://github.com/" target="_blank" rel="noopener">github</a> 根据相关信息注册账号(sign up 表示注册，sign in 表示登录)</li>
</ul>
<p>当需要注册organization的时候，则可以如图，选择new organization 来创建一个workgroup。<br><img src="https://github.com/leliyliu/figure_lib/blob/master/loognson/git/organization.png?raw=true" alt="workgourp"></p>
<h3 id="创建仓库-new-repository"><a href="#创建仓库-new-repository" class="headerlink" title="创建仓库(new repository)"></a>创建仓库(new repository)</h3><p>其中包括了几项内容，需要填写或者选择：repository name , Description, initialization or not , gitignore and license.</p>
<p><img src="https://github.com/leliyliu/figure_lib/blob/master/loognson/git/create.jpg?raw=true" alt="create"><br>选择clone or download,然后在本地打开git bash ，然后git clone：</p>
<p><img src="https://github.com/leliyliu/figure_lib/blob/master/loognson/git/clone.jpg?raw=true" alt="clone"></p>
<p><img src="https://github.com/leliyliu/figure_lib/blob/master/loognson/git/gitclone.jpg?raw=true" alt="git_clone"></p>
<h3 id="配置用户名和邮箱"><a href="#配置用户名和邮箱" class="headerlink" title="配置用户名和邮箱"></a>配置用户名和邮箱</h3><blockquote>
<p>第一次使用gitBash需要配置邮箱和用户名，邮箱可以填你自己的邮箱，用户名可以任意写，这不是做登录使用，就是保留自己的信息而已。</p>
</blockquote>
<ul>
<li>输入git config —global user.email按回车，然后输入你的邮箱</li>
<li>输入git config —global user.name按回车，然后输入你的用户名</li>
<li>邮箱和用户名都使用之后就可以正式使用git了</li>
</ul>
<h3 id="git-常用命令汇总"><a href="#git-常用命令汇总" class="headerlink" title="git 常用命令汇总"></a>git 常用命令汇总</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">克隆代码：git <span class="built_in">clone</span> 远程仓库的url</span><br><span class="line">配置邮箱：git config --global user.email</span><br><span class="line">配置用户名：git config --global user.name</span><br><span class="line">从远程仓库下拉代码到本地：git pull</span><br><span class="line">将本地代码添加到缓冲区：git add * .</span><br><span class="line">将本地代码提交到本地仓库：git commit -m<span class="string">"日志文字"</span></span><br><span class="line">将本地仓库同步到远程仓库：git push origin master</span><br><span class="line">查看日志：git <span class="built_in">log</span></span><br><span class="line">查看某个文件的提交日志：git <span class="built_in">log</span> 文件名</span><br><span class="line">查看某个用户的提交日志：git <span class="built_in">log</span> --author=“author”</span><br><span class="line">查看某条提交日志相信信息：git show 版本号</span><br><span class="line">查看git全部命令：git --<span class="built_in">help</span></span><br><span class="line">查看git某个命令的使用：git <span class="built_in">help</span> 命令名</span><br></pre></td></tr></table></figure>
<h3 id="more-commands"><a href="#more-commands" class="headerlink" title="more commands"></a>more commands</h3><p>参考<a href="https://www.jianshu.com/p/cf1e883d69d6" target="_blank" rel="noopener">Git 命令大全</a></p>
<h4 id="项目提交和拉取"><a href="#项目提交和拉取" class="headerlink" title="项目提交和拉取"></a>项目提交和拉取</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 克隆远程仓库到本地(先<span class="built_in">cd</span>到指定文件夹下,再执行<span class="built_in">clone</span>操作）</span><br><span class="line">$ git <span class="built_in">clone</span> git@github.com:leliyliu/augmips.git</span><br><span class="line"></span><br><span class="line">// 添加本地库</span><br><span class="line">$ git add ProjectName         --------  <span class="string">"ProjectName"</span>项目名称</span><br><span class="line"></span><br><span class="line">// 添加提交日志</span><br><span class="line">$ git commit -m ‘Journal’     </span><br><span class="line"></span><br><span class="line">// 提交到远端</span><br><span class="line">$ git push origin master      --------  <span class="string">"master"</span>分支名</span><br><span class="line"></span><br><span class="line">// 从远端拉取代</span><br><span class="line">$ git pull origin master</span><br></pre></td></tr></table></figure>
<h4 id="常用辅助命令"><a href="#常用辅助命令" class="headerlink" title="常用辅助命令"></a>常用辅助命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 查看提交日志</span><br><span class="line">$ git <span class="built_in">log</span> -3  --------查看最近三次提交的记录</span><br><span class="line"></span><br><span class="line">// 查看文件提交状态</span><br><span class="line">$ git status</span><br></pre></td></tr></table></figure>
<h4 id="标签操作"><a href="#标签操作" class="headerlink" title="标签操作"></a>标签操作</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 打一个新标签</span><br><span class="line">$ git tag v1.0.0</span><br><span class="line"></span><br><span class="line">// 查看所有标签</span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line">// 根据commit id 打标签</span><br><span class="line">$ git tag v1.0.1 2fbb88d5fad5a03e5b4ef2df316b4d32f5339ef5</span><br><span class="line"></span><br><span class="line">// 删除标签</span><br><span class="line">$ git tag -d v1.1.0</span><br></pre></td></tr></table></figure>
<h4 id="关于分支"><a href="#关于分支" class="headerlink" title="关于分支"></a>关于分支</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//1、 查看当前所有分支</span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># * dev          ---- 当前分支</span></span><br><span class="line"><span class="comment">#   master       ---- 主分支</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//2、 创建分支</span><br><span class="line">$ git branch dev</span><br><span class="line"></span><br><span class="line">//3、 切换分支</span><br><span class="line">$ git checkout dev</span><br><span class="line"></span><br><span class="line">//4、 创建并切换分支（相当于前两步操作）</span><br><span class="line">$ git checkout -b dev</span><br><span class="line"></span><br><span class="line">//5.1、 合并分支</span><br><span class="line">$ git merge dev                 ------ ⚠️ （此命令为：合并某分支到当前分支，例：将dev合并到master），则当前所处于master分支。</span><br><span class="line"></span><br><span class="line">//5.2、 合并dev分支上的某条记录到master上</span><br><span class="line">//（例如：在dev上有三次提交记录，但是只想把其中的第二次合并到master上去，采取这个命令）</span><br><span class="line">git cherry-pick <span class="string">"ddec59e2..."</span>   ------⚠️ 在master分支下输入命令，参数为 commit-id</span><br><span class="line"></span><br><span class="line">// 如果在合并中存在以下冲突，进入文件夹手动解决冲突再次提交</span><br><span class="line">hint: after resolving the conflicts, mark the corrected paths</span><br><span class="line">hint: with <span class="string">'git add &lt;paths&gt;'</span> or <span class="string">'git rm &lt;paths&gt;'</span></span><br><span class="line"></span><br><span class="line">//6、 删除指定分支</span><br><span class="line">$ git branch -D dev              ------ ⚠️（保证当前分支为非删除分支）</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//1、 查看当前所有分支</span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># * dev          ---- 当前分支</span></span><br><span class="line"><span class="comment">#   master       ---- 主分支</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//2、 创建分支</span><br><span class="line">$ git branch dev</span><br><span class="line"></span><br><span class="line">//3、 切换分支</span><br><span class="line">$ git checkout dev</span><br><span class="line"></span><br><span class="line">//4、 创建并切换分支（相当于前两步操作）</span><br><span class="line">$ git checkout -b dev</span><br><span class="line"></span><br><span class="line">//5.1、 合并分支</span><br><span class="line">$ git merge dev                 ------ ⚠️ （此命令为：合并某分支到当前分支，例：将dev合并到master），则当前所处于master分支。</span><br><span class="line"></span><br><span class="line">//5.2、 合并dev分支上的某条记录到master上</span><br><span class="line">//（例如：在dev上有三次提交记录，但是只想把其中的第二次合并到master上去，采取这个命令）</span><br><span class="line">git cherry-pick <span class="string">"ddec59e2..."</span>   ------⚠️ 在master分支下输入命令，参数为 commit-id</span><br><span class="line"></span><br><span class="line">// 如果在合并中存在以下冲突，进入文件夹手动解决冲突再次提交</span><br><span class="line">hint: after resolving the conflicts, mark the corrected paths</span><br><span class="line">hint: with <span class="string">'git add &lt;paths&gt;'</span> or <span class="string">'git rm &lt;paths&gt;'</span></span><br><span class="line"></span><br><span class="line">//6、 删除指定分支</span><br><span class="line">$ git branch -D dev              ------ ⚠️（保证当前分支为非删除分支）</span><br></pre></td></tr></table></figure>
<h4 id="版本的回退"><a href="#版本的回退" class="headerlink" title="版本的回退"></a>版本的回退</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span> -3</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit 65e19bef8eed336aefedaca636d78e71cea1d4f6 (HEAD -&gt; master, origin/master, origin/HEAD)</span></span><br><span class="line"><span class="comment"># Author: lizhiqiang &lt;601623654@qq.com&gt;</span></span><br><span class="line"><span class="comment"># Date:   Wed Mar 14 15:54:06 2018 +0800</span></span><br><span class="line"></span><br><span class="line">    增加1.1.3版本</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit 2fbb88d5fad5a03e5b4ef2df316b4d32f5339ef5</span></span><br><span class="line"><span class="comment"># Author: lizhiqiang &lt;601623654@qq.com&gt;</span></span><br><span class="line"><span class="comment"># Date:   Wed Mar 14 15:51:22 2018 +0800</span></span><br><span class="line"></span><br><span class="line">    增加1.1.2版本</span><br><span class="line"></span><br><span class="line"><span class="comment"># commit aa0f5ce90153c76249a12b8fa638a42b5155612b</span></span><br><span class="line"><span class="comment"># Author: lizhiqiang &lt;601623654@qq.com&gt;</span></span><br><span class="line"><span class="comment"># Date:   Wed Mar 14 15:46:03 2018 +0800</span></span><br><span class="line"></span><br><span class="line">    增加1.1.1版本</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 退回到上一个版本</span><br><span class="line">$ git reset --hard HEAD^</span><br><span class="line"><span class="comment"># HEAD is now at 2fbb88d 增加1.1.2版本    </span></span><br><span class="line">-----在此我们可以看到，HEAD指针已经由1.1.3版本指向了1.1.2版本，如果你把xcode打开就可以看到，1.1.3的代码已经消失了</span><br><span class="line">-----如果你的命令窗口没有关闭的话，通过commit id 还是可以返回1.1.3的,具体请看下一条命令</span><br><span class="line"></span><br><span class="line">// 返回任意版本</span><br><span class="line">$ git reset --hard 65e19bef8eed336aefedaca636d78e71cea1d4f6</span><br><span class="line"><span class="comment"># HEAD is now at 65e19be 增加1.1.3版本</span></span><br><span class="line"></span><br><span class="line">// 将修改后的版本推送至服务器</span><br><span class="line">$ git push -f -u origin master</span><br></pre></td></tr></table></figure>
<h4 id="阶段的撤销更改"><a href="#阶段的撤销更改" class="headerlink" title="阶段的撤销更改"></a>阶段的撤销更改</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// 已修改，未暂存   ------------没有执行 git add</span><br><span class="line">$ git checkout .    （或 $ git reset --hard）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 已暂存，未提交   ------------你已经执行了 git add . ，但还没有执行 git commit -m <span class="string">"comment"</span> 。</span><br><span class="line">$ git reset</span><br><span class="line">$ git checkout .     (或 $ git reset --hard)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 已提交，未推送   ------------既执行了 git add . ，又执行了 git commit </span><br><span class="line">$ git reset --hard origin/master</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 已推送          ------------既 git add 了，又 git commit 了，并且还 git push 了</span><br><span class="line">$ git reset --hard HEAD^</span><br><span class="line">$ git push -f</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2019/05/05/django-learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/05/django-learning/" itemprop="url">django_learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-05T14:56:16+08:00">
                2019-05-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2019/05/05/paper-reading/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/05/paper-reading/" itemprop="url">paper_reading</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-05T08:07:26+08:00">
                2019-05-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Neural-Collaborative-Filtering"><a href="#Neural-Collaborative-Filtering" class="headerlink" title="Neural Collaborative Filtering"></a>Neural Collaborative Filtering</h1><p>这篇文章主要介绍了深度学习在推荐系统方面的应用，主要是应用了神经网络搭建协同过滤的网络，形成一个较好的结果。</p>
<p>我将结合文章中所提及的主要方法及其代码来回顾整篇文章。</p>
<h2 id="3-NEURAL-COLLABORATIVE-FILTERING"><a href="#3-NEURAL-COLLABORATIVE-FILTERING" class="headerlink" title="3. NEURAL COLLABORATIVE FILTERING"></a>3. NEURAL COLLABORATIVE FILTERING</h2><p>从第三部分才来到实践的关键，所以我们直接跳过前两个部分，来到第三部分，并对它进行学习和理解。论文的主要部分在于，首先实现NCF，并对于NCF实例化之后利用DNN对其进行改善，提出了MLP，然后结合NCF和MLP来综合形成最后的结果。</p>
<h3 id="GMF"><a href="#GMF" class="headerlink" title="GMF"></a>GMF</h3><p>首先的input层是一个one-hot向量，将每一个user和每一个item都设置为one-hot向量之后再进行处理。在input 层之后是一个embedding 层，然后将item和user 的embedding层 的结果合在一起之后放入一个多层神经网络中，我们将这个多层的神经网络视作神经协同过滤层，隐藏层的最后一层的维度决定了这个模型的泛化能力。最终的输出层(output)得到的是最终预测的结果。</p>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>关于range 和 xrange，如果只看效果的话，两者差距很小，但是xrange每次返回的是xrange 的一个数据结构，而不是range返回的list，因此，当对于大数据进行处理时，建议使用xrange，而不是range。</p>
<p>关于代码中的参数说明：<br>| 符号| 含义|<br>| —- | —- |<br>| num_factors|将其映射到的空间的维度大小|<br>|regs|即正则化过程中的偏移量|<br>|num_neg|展示的negtive数量|<br>|lr|学习率|<br>|learner|学习的方式，一般来说使用adam|<br>|verbose|实质上是设置多少次迭代后输出结果|<br>|out| 设置输出|<br>|epochs| 迭代次数（学习次数）|<br>|batch_size|每一个batch的数量，使用sgd进行训练时需要弄清楚|</p>
<p>重点看model函数，即训练的函数。<br>这里的模型极其简单，实际上加深模型，应该有助于最后的训练结果。加深应该在哪里呢？从predict_vector开始，多加入几层，同时应该考虑不要社会太少的latent_dim，毕竟是大数据量的结果，考虑将其加深。原文中主要是为了能使得CPU较快的运行，才使用了这样一种方式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">(num_users, num_items, latent_dim, regs=[<span class="number">0</span>,<span class="number">0</span>])</span>:</span></span><br><span class="line">    <span class="comment"># Input variables</span></span><br><span class="line">    user_input = Input(shape=(<span class="number">1</span>,), dtype=<span class="string">'int32'</span>, name = <span class="string">'user_input'</span>)</span><br><span class="line">    item_input = Input(shape=(<span class="number">1</span>,), dtype=<span class="string">'int32'</span>, name = <span class="string">'item_input'</span>)</span><br><span class="line"></span><br><span class="line">    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = <span class="string">'user_embedding'</span>,</span><br><span class="line">                                  init = init_normal, W_regularizer = l2(regs[<span class="number">0</span>]), input_length=<span class="number">1</span>)</span><br><span class="line">    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = <span class="string">'item_embedding'</span>,</span><br><span class="line">                                  init = init_normal, W_regularizer = l2(regs[<span class="number">1</span>]), input_length=<span class="number">1</span>)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Crucial to flatten an embedding vector!</span></span><br><span class="line">    user_latent = Flatten()(MF_Embedding_User(user_input))</span><br><span class="line">    item_latent = Flatten()(MF_Embedding_Item(item_input))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Element-wise product of user and item embeddings </span></span><br><span class="line">    predict_vector = merge([user_latent, item_latent], mode = <span class="string">'mul'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Final prediction layer</span></span><br><span class="line">    <span class="comment">#prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)</span></span><br><span class="line">    prediction = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, init=<span class="string">'lecun_uniform'</span>, name = <span class="string">'prediction'</span>)(predict_vector)</span><br><span class="line">    </span><br><span class="line">    model = Model(input=[user_input, item_input], </span><br><span class="line">                output=prediction)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>get_train_instances函数将训练集传入并对其进行处理，构成one-hot向量。<br>但是实际上这里的label只是用于论文中所说的是否进行点击，对于后续的操作的意义不大。需要注意这里的区别。实际上，如果真正需要做评分系统的话，那么就需要将label定义为打分的结果，而不是0,1的二值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_instances</span><span class="params">(train, num_negatives)</span>:</span></span><br><span class="line">    user_input, item_input, labels = [],[],[]</span><br><span class="line">    num_users = train.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> (u, i) <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="comment"># positive instance</span></span><br><span class="line">        user_input.append(u)</span><br><span class="line">        item_input.append(i)</span><br><span class="line">        labels.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># negative instances</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> xrange(num_negatives):</span><br><span class="line">            j = np.random.randint(num_items)<span class="comment">#从0到item_num的数量中选择一个</span></span><br><span class="line">            <span class="keyword">while</span> train.has_key((u, j)):</span><br><span class="line">                j = np.random.randint(num_items)</span><br><span class="line">            user_input.append(u)</span><br><span class="line">            item_input.append(j)</span><br><span class="line">            labels.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> user_input, item_input, labels</span><br></pre></td></tr></table></figure>
<p>自己的尝试，主要用于大数据作业的数据集，简要介绍一下这个数据集：</p>
<ol>
<li>大概有20000个用户和600000个item，同时，总共有5000000条的打分，打分从0-100，但是分布极其不均匀，因此，需要对其进行处理，为了避免与稀疏矩阵的重合，将score变为:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score=np.floor(score/<span class="number">10</span>)+<span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>因此，构建的模型为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">(num_users, num_items, latent_dim=<span class="number">100</span>, regs=[<span class="number">0</span>,<span class="number">0</span>])</span>:</span></span><br><span class="line">    <span class="comment"># Input variables</span></span><br><span class="line">    user_input = Input(shape=(<span class="number">1</span>,), dtype=<span class="string">'int32'</span>, name = <span class="string">'user_input'</span>)</span><br><span class="line">    item_input = Input(shape=(<span class="number">1</span>,), dtype=<span class="string">'int32'</span>, name = <span class="string">'item_input'</span>)</span><br><span class="line"></span><br><span class="line">    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = <span class="string">'user_embedding'</span>,</span><br><span class="line">                                  init = <span class="string">'uniform'</span>, W_regularizer = l2(regs[<span class="number">0</span>]), input_length=<span class="number">1</span>)</span><br><span class="line">    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = <span class="string">'item_embedding'</span>,</span><br><span class="line">                                  init = <span class="string">'uniform'</span>, W_regularizer = l2(regs[<span class="number">1</span>]), input_length=<span class="number">1</span>)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Crucial to flatten an embedding vector!</span></span><br><span class="line">    user_latent = Flatten()(MF_Embedding_User(user_input))</span><br><span class="line">    item_latent = Flatten()(MF_Embedding_Item(item_input))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Element-wise product of user and item embeddings </span></span><br><span class="line">    <span class="comment">#predict_vector = merge([user_latent, item_latent], mode = 'mul')</span></span><br><span class="line">    predict_vector = keras.layers.Multiply()([user_latent,item_latent])</span><br><span class="line">    <span class="comment"># Final prediction layer</span></span><br><span class="line">    <span class="comment">#prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)</span></span><br><span class="line">    predict_layer1=Dense(<span class="number">64</span>,activation=<span class="string">'sigmoid'</span>,init=<span class="string">'lecun_uniform'</span>,name=<span class="string">'predict_layer1'</span>)(predict_vector)</span><br><span class="line">    predict_layer2=Dense(<span class="number">32</span>,activation=<span class="string">'sigmoid'</span>,init=<span class="string">'lecun_uniform'</span>,name=<span class="string">'predict_layer2'</span>)(predict_layer1)</span><br><span class="line">    prediction = Dense(<span class="number">11</span>, activation=<span class="string">'softmax'</span>, init=<span class="string">'lecun_uniform'</span>, name = <span class="string">'prediction'</span>)(predict_layer2)</span><br><span class="line">    </span><br><span class="line">    model = Model(input=[user_input, item_input], </span><br><span class="line">                output=prediction)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>中间多加了几层，对于报错处的记录</p>
<blockquote>
<p>‘Dense’ object has no attribute ‘outbound_nodes’</p>
</blockquote>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leliyliu.github.io/2019/05/04/colab/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leliyliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="禾声">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/04/colab/" itemprop="url">colab</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-04T17:10:58+08:00">
                2019-05-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Colab的使用"><a href="#Colab的使用" class="headerlink" title="Colab的使用"></a>Colab的使用</h1><hr>
<h2 id="Colab介绍"><a href="#Colab介绍" class="headerlink" title="Colab介绍"></a>Colab介绍</h2><p>好东西！！ 贫穷的人民，想拥有GPU或者TPU的难得的方式<br>对于一些基础的东西，都不加以介绍了，如果有兴趣，可以自己去查一下，或者ucadity的tensorflow教程里面就要用到这个东西，直接视频观看即可。<br>这里直接记录配置方式</p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p>首先选择修改，改为GPU或者TPU</p>
<p>然后添加如下代码后执行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">!apt-get install -y -qq software-properties-common python-software-properties module-init-tools</span><br><span class="line">!add-apt-repository -y ppa:alessandro-strada/ppa <span class="number">2</span>&gt;&amp;<span class="number">1</span> &gt; /dev/null</span><br><span class="line">!apt-get update -qq <span class="number">2</span>&gt;&amp;<span class="number">1</span> &gt; /dev/null</span><br><span class="line">!apt-get -y install -qq google-drive-ocamlfuse fuse</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> auth</span><br><span class="line">auth.authenticate_user()</span><br><span class="line"><span class="keyword">from</span> oauth2client.client <span class="keyword">import</span> GoogleCredentials</span><br><span class="line">creds = GoogleCredentials.get_application_default()</span><br><span class="line"><span class="keyword">import</span> getpass</span><br><span class="line">!google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125; &lt; /dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> | grep URL</span><br><span class="line">vcode = getpass.getpass()</span><br><span class="line">!echo &#123;vcode&#125; | google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125;</span><br></pre></td></tr></table></figure></p>
<p>然后挂载<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p drive</span><br><span class="line">!google-drive-ocamlfuse drive</span><br></pre></td></tr></table></figure></p>
<p>如果需要安装包的时候，比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install http://download.pytorch.org/whl/cu80/torch<span class="number">-0.3</span><span class="number">.1</span>-cp36-cp36m-linux_x86_64.whl torchvision</span><br></pre></td></tr></table></figure></p>
<p>这样安装就ok了</p>
<p>当然在执行之前还需要更改目录：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">'drive/.../...'</span>)<span class="comment">#进入你希望进入的目录</span></span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Leliyliu</p>
              <p class="site-description motion-element" itemprop="description">record</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">36</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leliyliu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
