#### Conferences

- <strong>Lian Liu</strong>, Shixin Zhao, Bing Li, Haimeng Ren, Zhaohui Xu, Mengdi Wang, Xiaowei Li, Yinhe Han, and Ying Wang*. "Make LLM Inference Afforable to Everyone: Augmenting GPU Memory with NDP-DIMM" in <strong> International Symposium on High-Performance Computer Architecture (HPCA'25), 2025. (CCF-A).</strong> 

- <strong>Lian Liu</strong>, Zhaohui Xu, Yintao He, Ying Wang*, Huawei Li, Xiaowei Li, Yinhe Han. "Drift: Leveraging Distribution-based Dynamic Precision Quantization for Efficient Deep Neural Network Acceleration" in <strong> Design Automation Conference (DAC'24), 2024. (CCF-A)</strong>. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3649329.3655986)

- Guanchen Li, Yixing Xu, Lian Liu, Xiandong Zhao, Dong Li. "Enhanced One-Shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism" in <strong> International Conference on
Computational Linguistics (COLING'24), 2024. (CCF-B)</strong>

#### Journal 

- <strong>Lian Liu</strong>, Ying Wang*, Weiwei Chen, Xiandong Zhao, Huawei Li*, Xiaowei Li, Yinhe Han, "An Automatic Neural Network Architecture-and-Quantization Joint Optimization Framework for Efficient Model Inference," in <strong> IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2024. (CCF-A)</strong>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10342823/)

- Wen Li, Ying Wang, Cheng Liu, Yintao He, <strong>Lian Liu</strong>, Huawei Li, Xiaowei Li, "On-line Fault Protection for ReRAM-based neural networks", <strong>IEEE Transaction on Computers (TC), 2024. (CCF-A)</strong>. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9737421/)

#### Preprints

- <strong>Lian Liu</strong>, Haimeng Ren, Long Cheng, Zhaohui Xu, Yudong Pan, Mengdi Wang, Xiaowei Li, Yinhe Han, and Ying Wang*. "COMET: Towards Partical W4A4KAV4 LLMs Serving", 2024. [[Paper]](https://arxiv.org/abs/2410.12168)